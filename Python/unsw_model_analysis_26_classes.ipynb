{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import useful libraries for analysis and modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.tree import export_graphviz, DecisionTreeClassifier\n",
    "pd.options.mode.chained_assignment = None\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "# Filter all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of 26 devices in dataset\n",
    "classes = ['Dropcam', 'HP Printer', 'Netatmo Welcome', 'Withings Smart Baby Monitor', 'Netatmo weather station',\\\n",
    "           'Smart Things', 'Amazon Echo', 'Samsung SmartCam','TP-Link Day Night Cloud camera', 'Triby Speaker',\\\n",
    "              'Belkin Wemo switch', 'TP-Link Smart plug', 'PIX-STAR Photo-frame','Belkin wemo motion sensor',\\\n",
    "                     'Samsung Galaxy Tab', 'NEST Protect smoke alarm', 'Withings Smart scale', 'IPhone',\\\n",
    "                            'MacBook', 'Withings Aura smart sleep sensor','Light Bulbs LiFX Smart Bulb',\\\n",
    "                            'Blipcare Blood Pressure meter','iHome', 'Insteon Camera', 'Android Phone', 'Laptop']\n",
    "classes_df = pd.DataFrame(classes, columns=['class'])\n",
    "\n",
    "# list of all extracted features\n",
    "feats_all = [\"ip.len\",\"ip.ttl\",\"tcp.flags.syn\",\"tcp.flags.ack\",\"tcp.flags.push\",\"tcp.flags.fin\",\"tcp.flags.rst\",\\\n",
    "            \"tcp.flags.ece\",\"ip.proto\",\"srcport\",\"dstport\",\"ip.hdr_len\",\"tcp.window_size_value\",\"tcp.hdr_len\",\"udp.length\",\\\n",
    "            \"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Total\",\"UDP Len Min\",\"UDP Len Max\",\\\n",
    "                \"Flow IAT Min\",\"Flow IAT Max\",\"Flow IAT Mean\",\"Flow Duration\",\\\n",
    "                    \"SYN Flag Count\",\"ACK Flag Count\",\"PSH Flag Count\",\"FIN Flag Count\",\"RST Flag Count\",\"ECE Flag Count\"]\n",
    "\n",
    "# list of easy to compute online features - without means\n",
    "feats_easy = [\"ip.len\",\"ip.ttl\",\"tcp.flags.syn\",\"tcp.flags.ack\",\"tcp.flags.push\",\"tcp.flags.fin\",\"tcp.flags.rst\",\\\n",
    "            \"tcp.flags.ece\",\"ip.proto\",\"srcport\",\"dstport\",\"ip.hdr_len\",\"tcp.window_size_value\",\"tcp.hdr_len\",\"udp.length\",\\\n",
    "            \"Min Packet Length\",\"Max Packet Length\",\"Packet Length Total\",\"UDP Len Min\",\"UDP Len Max\",\\\n",
    "                \"Flow IAT Min\",\"Flow IAT Max\",\"Flow Duration\",\"SYN Flag Count\",\"ACK Flag Count\",\\\n",
    "                    \"PSH Flag Count\",\"FIN Flag Count\",\"RST Flag Count\",\"ECE Flag Count\"]\n",
    "\n",
    "feats_no_time = [\"ip.len\",\"ip.ttl\",\"tcp.flags.syn\",\"tcp.flags.ack\",\"tcp.flags.push\",\"tcp.flags.fin\",\"tcp.flags.rst\",\\\n",
    "            \"tcp.flags.ece\",\"ip.proto\",\"srcport\",\"dstport\",\"tcp.window_size_value\",\"tcp.hdr_len\",\"udp.length\",\\\n",
    "            \"Min Packet Length\",\"Max Packet Length\",\"Packet Length Total\",\\\n",
    "                \"SYN Flag Count\",\"ACK Flag Count\",\"PSH Flag Count\",\"FIN Flag Count\",\"RST Flag Count\",\"ECE Flag Count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to save trained model to pickle\"\"\"\n",
    "def save_model(RF, filename):\n",
    "    pickle.dump(RF, open(filename, 'wb'))\n",
    "\n",
    "\"\"\"\n",
    "Function to get labels and indices of the Test set.\n",
    "\"\"\"\n",
    "def get_test_labels(IoT_Test):\n",
    "    array_of_indices = []\n",
    "    unique_labels = IoT_Test[\"Label\"].unique()\n",
    "    for lab in unique_labels:\n",
    "        index = classes_df[classes_df['class'] == lab].index.values[0]\n",
    "        array_of_indices.append(index)\n",
    "    return unique_labels, array_of_indices\n",
    "\n",
    "\"\"\"\n",
    "Function to Fit model based on optimal values of depth and number of estimators and use it\n",
    "to compute feature importance for all the features.\n",
    "\"\"\"\n",
    "def get_feature_importance(depth, n_tree, max_leaf, X_train, y_train, weight_of_samples):\n",
    "    \n",
    "    rf_opt = RandomForestClassifier(max_depth = depth, n_estimators = n_tree, max_leaf_nodes=max_leaf, random_state=42, bootstrap=False,n_jobs=10)\n",
    "    rf_opt.fit(X_train, y_train, sample_weight=weight_of_samples)\n",
    "    feature_importance = pd.DataFrame(rf_opt.feature_importances_)\n",
    "    feature_importance.index = X_train.columns\n",
    "    feature_importance = feature_importance.sort_values(by=list(feature_importance.columns),axis=0,ascending=False)\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "\"\"\"\n",
    "Function to Fit model based on optimal values of depth and number of estimators and feature importance\n",
    "to find the fewest possible features to exceed the previously attained score with all selected features\n",
    "\"\"\"\n",
    "def get_fewest_features(depth, n_tree, max_leaf, importance):    \n",
    "    sorted_feature_names = importance.index\n",
    "    # print('sorted_feature_names: ', sorted_feature_names)\n",
    "    features = []\n",
    "    for f in range(1,len(sorted_feature_names)+1):\n",
    "        features.append(sorted_feature_names[0:f])\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_x_y_flow(Dataset, feats):    \n",
    "    X = Dataset[feats]\n",
    "    y = Dataset['Label'].replace(classes, range(len(classes)))\n",
    "    sample_nature = Dataset['sample_nature']\n",
    "    return X, y, sample_nature\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function to calculate the score of the joint model\n",
    "\"\"\"\n",
    "def get_scores(classes, depth, n_tree, feats, max_leaf, X_train, y_train, X_test, y_test, unique_labels,array_of_indices,weight_of_samples):\n",
    "    model = RandomForestClassifier(max_depth=depth, n_estimators = n_tree, max_leaf_nodes=max_leaf, n_jobs=10,\n",
    "                                    random_state=42, bootstrap=False)\n",
    "    \n",
    "    model.fit(X_train[feats], y_train, sample_weight=weight_of_samples)\n",
    "    y_pred = model.predict(X_test[feats])\n",
    "\n",
    "    y_test = [int(label) for label in y_test.values]\n",
    "    y_pred = [int(label) for label in y_pred]\n",
    "\n",
    "    class_report = classification_report(y_test, y_pred, labels=unique_labels, target_names=array_of_indices, output_dict = True)\n",
    "\n",
    "    macro_score = class_report['macro avg']['f1-score']\n",
    "    weighted_score = class_report['weighted avg']['f1-score']\n",
    "\n",
    "    return model, class_report, macro_score, weighted_score, y_pred\n",
    "\n",
    "\"\"\"\n",
    "Function to calculate the score of the joint model in terms of Packet-Level metric\n",
    "\"\"\"\n",
    "def expand_rows_and_get_scores(y_true, y_pred, sample_nature, multiply,unique_labels,array_of_indices):\n",
    "    expanded_y_true = []\n",
    "    expanded_y_pred = []\n",
    "    \n",
    "    for true_label, pred_label, nature, mult in zip(y_true, y_pred, sample_nature, multiply):\n",
    "        if nature == 'flw':\n",
    "            expanded_y_true.extend([true_label] * (mult+1))\n",
    "            expanded_y_pred.extend([pred_label] * (mult+1))\n",
    "        else:\n",
    "            expanded_y_true.append(true_label)\n",
    "            expanded_y_pred.append(pred_label)\n",
    "    \n",
    "    num_samples = len(expanded_y_true)\n",
    "\n",
    "    expanded_y_true = [int(label) for label in expanded_y_true]\n",
    "    expanded_y_pred = [int(label) for label in expanded_y_pred]\n",
    "    #\n",
    "    macro_f1 = classification_report(expanded_y_true, expanded_y_pred, labels=unique_labels, target_names=array_of_indices, output_dict=True)['macro avg']['f1-score']\n",
    "    weighted_f1 = classification_report(expanded_y_true, expanded_y_pred, output_dict=True)['weighted avg']['f1-score']\n",
    "    \n",
    "    return num_samples, macro_f1, weighted_f1\n",
    "\n",
    "\"\"\"\n",
    "Function to calculate the score of the joint model in classifying the flow and classifying the first N-1 packets\n",
    "\"\"\"\n",
    "def compute_flow_pkt_scores(y_pred, y_test, sample_nature,unique_labels,array_of_indices):\n",
    "\n",
    "    # Create a data frame with the three columns\n",
    "    df = pd.DataFrame({'y_pred': y_pred, 'y_test': y_test, 'sample_nature': sample_nature})\n",
    "    \n",
    "    # Split the data frame into two data frames based on sample_nature\n",
    "    pkt_df = df[df['sample_nature'] == 'pkt']\n",
    "    flw_df = df[df['sample_nature'] == 'flw']\n",
    "    \n",
    "    # Compute macro and weighted F1 scores for pkt_df\n",
    "    pkt_df_y_true = [int(label) for label in pkt_df['y_test'].values]\n",
    "    pkt_df_y_pred = [int(label) for label in pkt_df['y_pred']]\n",
    "\n",
    "    # labels=array_of_indices, target_names=unique_labels,\n",
    "    pkt_macro_f1 = classification_report(pkt_df_y_true, pkt_df_y_pred, labels=unique_labels, target_names=array_of_indices, output_dict=True)['macro avg']['f1-score']\n",
    "    pkt_weighted_f1 = classification_report(pkt_df_y_true, pkt_df_y_pred, labels=unique_labels, target_names=array_of_indices, output_dict=True)['weighted avg']['f1-score']\n",
    "    \n",
    "    # Compute macro and weighted F1 scores for flw_df\n",
    "    flw_df_y_true = [int(label) for label in flw_df['y_test'].values]\n",
    "    flw_df_y_pred = [int(label) for label in flw_df['y_pred']]\n",
    "\n",
    "    flw_macro_f1 = classification_report(flw_df_y_true, flw_df_y_pred, labels=unique_labels, target_names=array_of_indices, output_dict=True)['macro avg']['f1-score']\n",
    "    flw_weighted_f1 = classification_report(flw_df_y_true, flw_df_y_pred, labels=unique_labels, target_names=array_of_indices, output_dict=True)['weighted avg']['f1-score']\n",
    "\n",
    "    return pkt_macro_f1, pkt_weighted_f1, flw_macro_f1, flw_weighted_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to check the conditions and detect if the packet is the Nth packet or\n",
    "before the Nth packet and assign values to clssify them accordingly\n",
    "'''\n",
    "def assign_sample_nature(row):\n",
    "    if (row[\"Min Packet Length\"] == -1 and\n",
    "        row[\"Max Packet Length\"] == -1 and\n",
    "        row[\"Flow IAT Min\"] == -1 and\n",
    "        row[\"Flow IAT Max\"] == -1):\n",
    "        return \"pkt\"\n",
    "    else:\n",
    "        return \"flw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Function for grid search on hyperparameters (depth, number of trees, number of features, maximum number of leaves), features and models with a given N value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze models\n",
    "def analyze_models(classes, model_type, depths, n_trees, X_train, y_train, X_test, y_test, samples_nature, y_multiply, max_leaf,  test_labels, test_indices, max_feats, filename, weight_of_samples):\n",
    "    #open file to save ouput of analysis\n",
    "    with open(filename, \"w\") as res_file:\n",
    "        print('depth;tree;no_feats;pkt_macro_f1;pkt_weighted_f1;flw_macro_f1;flw_weighted_f1;F1_macro;F1_weighted;num_samples;PL_Macro_F1;PL_Weighted_F1;N_Leaves;feats', file=res_file)\n",
    "        if model_type == 'RF':\n",
    "            # FOR EACH (depth, n_tree, feat)\n",
    "            for depth in depths:\n",
    "                for n_tree in n_trees:\n",
    "                    for leaf in max_leaf:\n",
    "                        # get feature orders to use (ordered in terms of feature importances)\n",
    "                        importance = get_feature_importance(depth, n_tree, leaf, X_train, y_train, weight_of_samples)\n",
    "                        importance = importance[0:max_feats]\n",
    "                        m_feats = get_fewest_features(depth, n_tree, leaf, importance) \n",
    "                        for feats in m_feats:\n",
    "                            # GET all the scores\n",
    "                            model, c_report, macro_f1, weight_f1, y_pred = get_scores(classes, depth, n_tree, feats, leaf, X_train, y_train, X_test, y_test,  test_indices, test_labels, weight_of_samples)\n",
    "                            #\n",
    "                            pkt_macro_f1, pkt_weighted_f1, flw_macro_f1, flw_weighted_f1 = compute_flow_pkt_scores(y_pred, y_test, samples_nature, test_indices, test_labels)\n",
    "                            #\n",
    "                            num_samples, PL_macro_f1, PL_weighted_f1 = expand_rows_and_get_scores(y_test, y_pred, samples_nature, y_multiply, test_indices, test_labels)\n",
    "                            #\n",
    "                            # Save the model analysis results\n",
    "                            print(str(depth)+';'+str(n_tree)+';'+str(len(feats))+';'+str(pkt_macro_f1)+';'+str(pkt_weighted_f1)+';'+str(flw_macro_f1)+';'+str(flw_weighted_f1)+';'+str(macro_f1)+';'+str(weight_f1)+';'+str(num_samples)+';'+str(PL_macro_f1)+';'+str(PL_weighted_f1)+';'+str(leaf)+\";\"+str(list(feats)), file=res_file)\n",
    "    print(\"Analysis Complete. Check output file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model Analysis - Flows with the first n packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes desired number of packets (N), the output file to store values, and the feature list to use in the model analysis\n",
    "def analyze_model_n_packets(npkts, outfile, feats_to_use):    \n",
    "\n",
    "    # Load Train and Test data\n",
    "    train_data = pd.read_csv(\"train_data_\"+str(npkts)+\"_pkts.csv\")\n",
    "    test_data = pd.read_csv(\"test_data_\"+str(npkts)+\"_pkts.csv\")\n",
    "    #\n",
    "    #\n",
    "    # Generate a dataframe with the information of number of packets of each flow in Test Data\n",
    "    flow_pkt_counts = pd.read_csv(\"test_data_flow_packet_counts.csv\")\n",
    "    flow_count_dict = flow_pkt_counts.set_index(\"flow.id\")[\"count\"].to_dict()\n",
    "    # Map the values from flow_pkt_counts to test_data based on the \"Flow ID\" column\n",
    "    test_data[\"pkt_count\"] = test_data[\"Flow ID\"].map(flow_count_dict)\n",
    "    #\n",
    "    all_minus_one = (test_data['Min Packet Length'] == -1) & (test_data['Max Packet Length'] == -1) & (test_data['Packet Length Mean'] == -1)\n",
    "    # Assign values to the multiply column based on the conditions\n",
    "    test_data['multiply'] = np.where(all_minus_one, 1, test_data['pkt_count'] - npkts)\n",
    "    #\n",
    "    test_labels, test_indices = get_test_labels(test_data)\n",
    "    print(\"Num Labels: \", len(test_labels))\n",
    "    #\n",
    "    #\n",
    "    train_data = train_data.sample(frac=1, random_state=42)\n",
    "    test_data  = test_data.sample(frac=1, random_state=42)\n",
    "\n",
    "    train_data = train_data.dropna(subset=['srcport', 'dstport']) \n",
    "    test_data  = test_data.dropna(subset=['srcport', 'dstport'])\n",
    "    #\n",
    "    # Assign 'pkt' to all the packets before Nth packet and 'flw' to all Nth packets to classify them accordingly.\n",
    "    train_data['sample_nature'] = train_data.apply(assign_sample_nature, axis=1)\n",
    "    test_data['sample_nature']  = test_data.apply(assign_sample_nature, axis=1)\n",
    "    \n",
    "    # Assign weights to the packets for training. \n",
    "    ## 1: if the packet is before Nth  N: If the packet is the Nth packets (N: the rank of the packet for which the Flow-Level inference is triggered.)\n",
    "    train_data['weight'] = np.where(train_data['sample_nature'] == 'flw', npkts, 1)\n",
    "    weight_of_samples = list(train_data['weight'])\n",
    "\n",
    "    # Get Variables and Labels\n",
    "    y_multiply = test_data['multiply'].astype(int)\n",
    "    X_train, y_train, sample_nat_train = get_x_y_flow(train_data, feats_to_use)\n",
    "    X_test,  y_test, sample_nat_test  = get_x_y_flow(test_data, feats_to_use)\n",
    "\n",
    "    leaves   = [41,85,129,173,217,261,305,349,393,437,481,500]\n",
    "    depths   = [9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    trees    = [1,2,3,4,5]\n",
    "    max_feat = 10\n",
    "\n",
    "    # Run model analysis with the given search space (depth, number of trees, number of features, maximum number of leaves, and the value of 'N')\n",
    "    analyze_models(classes, \"RF\", depths, trees, X_train, y_train, X_test, y_test, sample_nat_test, y_multiply, leaves, test_labels, test_indices, max_feat, outfile, weight_of_samples)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run for different N values (N=2 to 10) - (N: the rank of the packet for which the Flow-Level inference is triggered.)\n",
    "for nd in range(2,10):\n",
    "    print(\"Number of Packets for Flow Features: \", nd)\n",
    "    f_name = \"unsw_models_results_\"+str(nd)+\"pkts.csv\"\n",
    "    analyze_model_n_packets(nd, f_name, feats_no_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the chosen model and save to later convert into M/A entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model size, N value, and list of the features that are used in the chosen model\n",
    "depth = 14\n",
    "n_of_trees = 3\n",
    "max_n_leaves = 481\n",
    "npkts = 3\n",
    "feats_to_use = ['tcp.window_size_value', 'ip.len', 'srcport', 'dstport', 'ip.ttl', 'tcp.hdr_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train and Test data\n",
    "train_data = pd.read_csv(\"train_data_\"+str(npkts)+\"_pkts.csv\")\n",
    "test_data = pd.read_csv(\"test_data_\"+str(npkts)+\"_pkts.csv\")\n",
    "#\n",
    "#\n",
    "# Generate a dataframe with the information of number of packets of each flow in Test Data\n",
    "flow_pkt_counts = pd.read_csv(\"test_data_flow_packet_counts\")\n",
    "flow_count_dict = flow_pkt_counts.set_index(\"flow.id\")[\"count\"].to_dict()\n",
    "# Map the values from flow_pkt_counts to test_data based on the \"Flow ID\" column\n",
    "test_data[\"pkt_count\"] = test_data[\"Flow ID\"].map(flow_count_dict)\n",
    "#\n",
    "all_minus_one = (test_data['Min Packet Length'] == -1) & (test_data['Max Packet Length'] == -1) & (test_data['Packet Length Mean'] == -1)\n",
    "# Assign values to the multiply column based on the conditions\n",
    "test_data['multiply'] = np.where(all_minus_one, 1, test_data['pkt_count'] - npkts)\n",
    "#\n",
    "test_labels, test_indices = get_test_labels(test_data)\n",
    "print(\"Num Labels: \", len(test_labels))\n",
    "#\n",
    "#\n",
    "train_data = train_data.sample(frac=1, random_state=42)\n",
    "test_data  = test_data.sample(frac=1, random_state=42)\n",
    "\n",
    "train_data = train_data.dropna(subset=['srcport', 'dstport']) \n",
    "test_data  = test_data.dropna(subset=['srcport', 'dstport'])\n",
    "#\n",
    "# Assign 'pkt' to all the packets before Nth packet and 'flw' to all Nth packets to classify them accordingly.\n",
    "train_data['sample_nature'] = train_data.apply(assign_sample_nature, axis=1)\n",
    "test_data['sample_nature']  = test_data.apply(assign_sample_nature, axis=1)\n",
    "\n",
    "# Assign weights to the packets for training. \n",
    "## 1: if the packet is before Nth  N: If the packet is the Nth packets (N: the rank of the packet for which the Flow-Level inference is triggered.)\n",
    "train_data['weight'] = np.where(train_data['sample_nature'] == 'flw', npkts, 1)\n",
    "weight_of_samples = list(train_data['weight'])\n",
    "\n",
    "# Get Variables and Labels\n",
    "y_multiply = test_data['multiply'].astype(int)\n",
    "X_train, y_train, sample_nat_train = get_x_y_flow(train_data, feats_to_use)\n",
    "X_test,  y_test, sample_nat_test  = get_x_y_flow(test_data, feats_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model\n",
    "model_unsw_jewel_14_3_6_N3, c_report, macro_f1, weight_f1, y_pred = get_scores(classes, depth, n_of_trees, feats_to_use, max_n_leaves, X_train, y_train, X_test, y_test,  test_indices, test_labels, weight_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for onward conversion into M/A entries\n",
    "save_model(model_unsw_jewel_14_3_6_N3, 'model_unsw_jewel_14_3_6_n3.sav')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
